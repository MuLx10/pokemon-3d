{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Pok√©mon Data With Apache Spark\n",
    "\n",
    "by Max Woolf (@minimaxir)\n",
    "\n",
    "*This notebook is licensed under the MIT License. If you use the code or data visualization designs contained within this notebook, it would be greatly appreciated if proper attribution is given back to this notebook and/or myself. Thanks! :)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web URL: local[*]:4040\n"
     ]
    }
   ],
   "source": [
    "# Easiest way to get Spark to work with Jupyter: https://github.com/minrk/findspark\n",
    "import findspark\n",
    "findspark.init('/Users/maxwoolf/Desktop/spark-2.0.0-bin-hadoop2.7')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "sc = pyspark.SparkContext(appName=\"pokemon_pca\")\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web URL: 10.0.1.11:4040\n"
     ]
    }
   ],
   "source": [
    "config = sc._conf.getAll()\n",
    "print 'Web URL: ' + filter(lambda x: 'spark.driver.host' in x[0], config)[0][1] + ':4040'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and cache all necessary DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_params = {'header': True, 'inferSchema': True}\n",
    "\n",
    "df_pokemon = spark.read.csv('pokemon_data/pokemon.csv', header=True, inferSchema=True).cache()\n",
    "df_stats = spark.read.csv('pokemon_data/pokemon_stats.csv', header=True, inferSchema=True).cache()\n",
    "df_types = spark.read.csv('pokemon_data/pokemon_types.csv', header=True, inferSchema=True).cache()\n",
    "df_moves = spark.read.csv('pokemon_data/pokemon_moves.csv', header=True, inferSchema=True).cache()\n",
    "df_abilities = spark.read.csv('pokemon_data/pokemon_abilities.csv', header=True, inferSchema=True).cache()\n",
    "df_species = spark.read.csv('pokemon_data/pokemon_species.csv', header=True, inferSchema=True).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Height/Weight\n",
    "\n",
    "Normalize Pokemon Height/Weight to [0,1] by dividing by Max Height/Weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+------+------+---------------+-----+----------+\n",
      "| id|identifier|species_id|height|weight|base_experience|order|is_default|\n",
      "+---+----------+----------+------+------+---------------+-----+----------+\n",
      "|  1| bulbasaur|         1|     7|    69|             64|    1|         1|\n",
      "|  2|   ivysaur|         2|    10|   130|            142|    2|         1|\n",
      "|  3|  venusaur|         3|    20|  1000|            236|    3|         1|\n",
      "|  4|charmander|         4|     6|    85|             62|    5|         1|\n",
      "|  5|charmeleon|         5|    11|   190|            142|    6|         1|\n",
      "|  6| charizard|         6|    17|   905|            240|    7|         1|\n",
      "+---+----------+----------+------+------+---------------+-----+----------+\n",
      "only showing top 6 rows\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- identifier: string (nullable = true)\n",
      " |-- species_id: integer (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      " |-- weight: integer (nullable = true)\n",
      " |-- base_experience: integer (nullable = true)\n",
      " |-- order: integer (nullable = true)\n",
      " |-- is_default: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pokemon.show(6)\n",
    "df_pokemon.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            height_t|            weight_t|\n",
      "+---+--------------------+--------------------+\n",
      "|  1| 0.04827586206896552|0.006902070621186...|\n",
      "|  2| 0.06896551724137931|0.013003901170351105|\n",
      "|  3| 0.13793103448275862| 0.10003000900270081|\n",
      "|  4|0.041379310344827586|0.008502550765229568|\n",
      "|  5| 0.07586206896551724|0.019005701710513155|\n",
      "|  6| 0.11724137931034483| 0.09052715814744423|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_height = df_pokemon.agg({\"height\": \"max\"}).first()[0]\n",
    "max_weight = df_pokemon.agg({\"weight\": \"max\"}).first()[0]\n",
    "\n",
    "df_pokemon_t = (df_pokemon\n",
    "                .filter(df_pokemon.id < 10000)\n",
    "              .withColumn('height_t', df_pokemon.height / max_height)\n",
    "              .withColumn('weight_t', df_pokemon.weight / max_weight)\n",
    "                .select(['id','height_t', 'weight_t'])\n",
    "              )\n",
    "\n",
    "df_pokemon_t.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Stats (Att/Def/etc.)\n",
    "\n",
    "Split each stat into its own column, then run the same scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+------+\n",
      "|pokemon_id|stat_id|base_stat|effort|\n",
      "+----------+-------+---------+------+\n",
      "|         1|      1|       45|     0|\n",
      "|         1|      2|       49|     0|\n",
      "|         1|      3|       49|     0|\n",
      "|         1|      4|       65|     1|\n",
      "|         1|      5|       65|     0|\n",
      "|         1|      6|       45|     0|\n",
      "|         2|      1|       60|     0|\n",
      "|         2|      2|       62|     0|\n",
      "|         2|      3|       63|     0|\n",
      "|         2|      4|       80|     1|\n",
      "|         2|      5|       80|     1|\n",
      "|         2|      6|       60|     0|\n",
      "+----------+-------+---------+------+\n",
      "only showing top 12 rows\n",
      "\n",
      "root\n",
      " |-- pokemon_id: integer (nullable = true)\n",
      " |-- stat_id: integer (nullable = true)\n",
      " |-- base_stat: integer (nullable = true)\n",
      " |-- effort: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stats.show(12)\n",
    "df_stats.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate each `base_stat` into an `array` of length 6 for each Pokemon. Requires stepping down to RDDs.\n",
    "\n",
    "Extract each row as a tuple, group by key to get array group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+------+\n",
      "|pokemon_id|stat_id|base_stat|effort|\n",
      "+----------+-------+---------+------+\n",
      "|         1|      1|       45|     0|\n",
      "|         1|      2|       49|     0|\n",
      "|         1|      3|       49|     0|\n",
      "|         1|      4|       65|     1|\n",
      "|         1|      5|       65|     0|\n",
      "|         1|      6|       45|     0|\n",
      "|         2|      1|       60|     0|\n",
      "|         2|      2|       62|     0|\n",
      "|         2|      3|       63|     0|\n",
      "|         2|      4|       80|     1|\n",
      "|         2|      5|       80|     1|\n",
      "|         2|      6|       60|     0|\n",
      "|         3|      1|       80|     0|\n",
      "|         3|      2|       82|     0|\n",
      "|         3|      3|       83|     0|\n",
      "|         3|      4|      100|     2|\n",
      "|         3|      5|      100|     1|\n",
      "|         3|      6|       80|     0|\n",
      "|         4|      1|       39|     0|\n",
      "|         4|      2|       52|     0|\n",
      "+----------+-------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stats.orderBy('pokemon_id', 'stat_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+\n",
      "| _1| _2| _3| _4| _5| _6| _7|\n",
      "+---+---+---+---+---+---+---+\n",
      "|  1| 45| 49| 49| 65| 65| 45|\n",
      "|  2| 60| 62| 63| 80| 80| 60|\n",
      "|  3| 80| 82| 83|100|100| 80|\n",
      "|  4| 39| 52| 43| 60| 50| 65|\n",
      "|  5| 58| 64| 58| 80| 65| 80|\n",
      "|  6| 78| 84| 78|109| 85|100|\n",
      "|  7| 44| 48| 65| 50| 64| 43|\n",
      "|  8| 59| 63| 80| 65| 80| 58|\n",
      "|  9| 79| 83|100| 85|105| 78|\n",
      "| 10| 45| 30| 35| 20| 20| 45|\n",
      "| 11| 50| 20| 55| 25| 25| 30|\n",
      "| 12| 60| 45| 50| 90| 80| 70|\n",
      "+---+---+---+---+---+---+---+\n",
      "only showing top 12 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stats_t = (df_stats\n",
    "                .orderBy('pokemon_id', 'stat_id')   # Ensure correct order\n",
    "                .rdd\n",
    "                .map(lambda row: (row[0], row[2]))\n",
    "                .groupByKey()\n",
    "                .map(lambda row: (row[0], list(row[1]))) \n",
    "                .map(lambda row: (row[0], row[1][0], row[1][1], row[1][2], row[1][3], row[1][4], row[1][5]))\n",
    "                .toDF()\n",
    "                .orderBy('_1')\n",
    "                .cache()\n",
    "                )\n",
    "\n",
    "df_stats_t.show(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the columns. Unfortunately, bulk renames annoying in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------+-------+--------------+---------------+-----+\n",
      "| id| hp|attack|defense|special_attack|special_defense|speed|\n",
      "+---+---+------+-------+--------------+---------------+-----+\n",
      "|  1| 45|    49|     49|            65|             65|   45|\n",
      "|  2| 60|    62|     63|            80|             80|   60|\n",
      "|  3| 80|    82|     83|           100|            100|   80|\n",
      "|  4| 39|    52|     43|            60|             50|   65|\n",
      "|  5| 58|    64|     58|            80|             65|   80|\n",
      "|  6| 78|    84|     78|           109|             85|  100|\n",
      "|  7| 44|    48|     65|            50|             64|   43|\n",
      "|  8| 59|    63|     80|            65|             80|   58|\n",
      "|  9| 79|    83|    100|            85|            105|   78|\n",
      "| 10| 45|    30|     35|            20|             20|   45|\n",
      "| 11| 50|    20|     55|            25|             25|   30|\n",
      "| 12| 60|    45|     50|            90|             80|   70|\n",
      "+---+---+------+-------+--------------+---------------+-----+\n",
      "only showing top 12 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# http://stackoverflow.com/a/34077809\n",
    "# via stats.csv\n",
    "col_names = df_stats_t.schema.names\n",
    "stat_names = ['id', 'hp', 'attack', 'defense', 'special_attack', 'special_defense', 'speed']\n",
    "\n",
    "\n",
    "df_stats_t = reduce(lambda data, idx: data.withColumnRenamed(col_names[idx], stat_names[idx]), xrange(len(col_names)), df_stats_t)\n",
    "df_stats_t.show(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale each column to [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "| id|           attack_t|          defense_t|   special_attack_t|  special_defense_t|            speed_t|\n",
      "+---+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|  1| 0.2578947368421053|0.21304347826086956|0.33505154639175255| 0.2826086956521739|               0.25|\n",
      "|  2| 0.3263157894736842|0.27391304347826084|0.41237113402061853|0.34782608695652173| 0.3333333333333333|\n",
      "|  3|0.43157894736842106|0.36086956521739133| 0.5154639175257731|0.43478260869565216| 0.4444444444444444|\n",
      "|  4| 0.2736842105263158|0.18695652173913044|0.30927835051546393|0.21739130434782608| 0.3611111111111111|\n",
      "|  5| 0.3368421052631579|0.25217391304347825|0.41237113402061853| 0.2826086956521739| 0.4444444444444444|\n",
      "|  6| 0.4421052631578947| 0.3391304347826087| 0.5618556701030928| 0.3695652173913043| 0.5555555555555556|\n",
      "|  7|0.25263157894736843| 0.2826086956521739|0.25773195876288657| 0.2782608695652174| 0.2388888888888889|\n",
      "|  8|0.33157894736842103|0.34782608695652173|0.33505154639175255|0.34782608695652173|0.32222222222222224|\n",
      "|  9| 0.4368421052631579|0.43478260869565216| 0.4381443298969072|0.45652173913043476|0.43333333333333335|\n",
      "| 10|0.15789473684210525|0.15217391304347827|0.10309278350515463|0.08695652173913043|               0.25|\n",
      "| 11|0.10526315789473684| 0.2391304347826087|0.12886597938144329|0.10869565217391304|0.16666666666666666|\n",
      "| 12|0.23684210526315788|0.21739130434782608| 0.4639175257731959|0.34782608695652173| 0.3888888888888889|\n",
      "| 13|0.18421052631578946|0.13043478260869565|0.10309278350515463|0.08695652173913043| 0.2777777777777778|\n",
      "| 14|0.13157894736842105|0.21739130434782608|0.12886597938144329|0.10869565217391304|0.19444444444444445|\n",
      "| 15|0.47368421052631576|0.17391304347826086|0.23195876288659795|0.34782608695652173| 0.4166666666666667|\n",
      "| 16|0.23684210526315788|0.17391304347826086|0.18041237113402062|0.15217391304347827| 0.3111111111111111|\n",
      "| 17| 0.3157894736842105| 0.2391304347826087|0.25773195876288657|0.21739130434782608|0.39444444444444443|\n",
      "| 18|0.42105263157894735|0.32608695652173914|0.36082474226804123|0.30434782608695654| 0.5611111111111111|\n",
      "| 19|0.29473684210526313|0.15217391304347827|0.12886597938144329|0.15217391304347827|                0.4|\n",
      "| 20| 0.4263157894736842| 0.2608695652173913|0.25773195876288657|0.30434782608695654| 0.5388888888888889|\n",
      "+---+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_hp = df_stats_t.selectExpr('max(hp)').first()[0]\n",
    "max_attack = df_stats_t.selectExpr('max(attack)').first()[0]\n",
    "max_defense = df_stats_t.selectExpr('max(defense)').first()[0]\n",
    "max_special_attack = df_stats_t.selectExpr('max(special_attack)').first()[0]\n",
    "max_special_defense = df_stats_t.selectExpr('max(special_defense)').first()[0]\n",
    "max_speed = df_stats_t.selectExpr('max(speed)').first()[0]\n",
    "\n",
    "df_stats_t = (df_stats_t\n",
    "              .withColumn('hp_t', df_stats_t.hp / max_hp)\n",
    "              .withColumn('attack_t', df_stats_t.attack / max_attack)\n",
    "              .withColumn('defense_t', df_stats_t.defense / max_defense)\n",
    "              .withColumn('special_attack_t', df_stats_t.special_attack / max_special_attack)\n",
    "              .withColumn('special_defense_t', df_stats_t.special_defense / max_special_defense)\n",
    "              .withColumn('speed_t', df_stats_t.speed / max_speed)\n",
    "              .select(['id', 'attack_t', 'defense_t', 'special_attack_t', 'special_defense_t', 'speed_t'])\n",
    "              )\n",
    "\n",
    "df_stats_t.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarize\n",
    "\n",
    "Calculate binary features for the rest of the datasets. Violates DRY heavily, but cannot be helped due to quirks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----+\n",
      "|pokemon_id|type_id|slot|\n",
      "+----------+-------+----+\n",
      "|         1|     12|   1|\n",
      "|         1|      4|   2|\n",
      "|         2|     12|   1|\n",
      "|         2|      4|   2|\n",
      "|         3|     12|   1|\n",
      "|         3|      4|   2|\n",
      "|         4|     10|   1|\n",
      "|         5|     10|   1|\n",
      "|         6|     10|   1|\n",
      "|         6|      3|   2|\n",
      "|         7|     11|   1|\n",
      "|         8|     11|   1|\n",
      "+----------+-------+----+\n",
      "only showing top 12 rows\n",
      "\n",
      "root\n",
      " |-- pokemon_id: integer (nullable = true)\n",
      " |-- type_id: integer (nullable = true)\n",
      " |-- slot: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_types.show(12)\n",
    "df_types.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDs must be zero-indexed for `OneHotEncoder`. Since IDs are just one-indexed, we can just subtract 1 from the IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|pokemon_id|type_t|\n",
      "+----------+------+\n",
      "|         1|    11|\n",
      "|         1|     3|\n",
      "|         2|    11|\n",
      "|         2|     3|\n",
      "|         3|    11|\n",
      "|         3|     3|\n",
      "|         4|     9|\n",
      "|         5|     9|\n",
      "|         6|     9|\n",
      "|         6|     2|\n",
      "|         7|    10|\n",
      "|         8|    10|\n",
      "+----------+------+\n",
      "only showing top 12 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_types_t = df_types.select('pokemon_id', (df_types.type_id-1).alias('type_t'))\n",
    "\n",
    "df_types_t.show(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run OHE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#from pyspark.ml.feature import OneHotEncoder\n",
    "#\n",
    "#ohe = OneHotEncoder(inputCol=\"type_t\", outputCol=\"type_v\")\n",
    "#df_types_t = ohe.transform(df_types_t)\n",
    "#\n",
    "#df_types_t.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a pseudo-`OneHotEncoder` by specifying the type indices directly to the `SparseVector`. This approach does not drop the last category, which is acceptable since we are not generating a predictive model.\n",
    "\n",
    "If a Pokemon has 2 types, there are two values/indices in each `SparseVector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|              type_t|\n",
      "+---+--------------------+\n",
      "|  1|(18,[3,11],[1.0,1...|\n",
      "|  2|(18,[3,11],[1.0,1...|\n",
      "|  3|(18,[3,11],[1.0,1...|\n",
      "|  4|      (18,[9],[1.0])|\n",
      "|  5|      (18,[9],[1.0])|\n",
      "|  6|(18,[2,9],[1.0,1.0])|\n",
      "|  7|     (18,[10],[1.0])|\n",
      "|  8|     (18,[10],[1.0])|\n",
      "|  9|     (18,[10],[1.0])|\n",
      "| 10|      (18,[6],[1.0])|\n",
      "| 11|      (18,[6],[1.0])|\n",
      "| 12|(18,[2,6],[1.0,1.0])|\n",
      "| 13|(18,[3,6],[1.0,1.0])|\n",
      "| 14|(18,[3,6],[1.0,1.0])|\n",
      "| 15|(18,[3,6],[1.0,1.0])|\n",
      "| 16|(18,[0,2],[1.0,1.0])|\n",
      "| 17|(18,[0,2],[1.0,1.0])|\n",
      "| 18|(18,[0,2],[1.0,1.0])|\n",
      "| 19|      (18,[0],[1.0])|\n",
      "| 20|      (18,[0],[1.0])|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# http://stackoverflow.com/questions/32981875/how-to-add-two-sparse-vectors-in-spark-using-python\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "\n",
    "# Given an array of indices, create a SparseVector\n",
    "\n",
    "def array_to_sparse(size, v):\n",
    "    values = {i: 1.0 for i in v}\n",
    "    return Vectors.sparse(size, values)\n",
    "\n",
    "sparse_size = df_types_t.selectExpr('count(distinct(type_t))').first()[0]\n",
    "\n",
    "df_types_t = (df_types_t\n",
    "            .rdd\n",
    "            .map(lambda row: (row[0], row[1]))\n",
    "            .groupByKey()\n",
    "            .map(lambda row: (row[0], array_to_sparse(sparse_size, row[1])))\n",
    "            .toDF()\n",
    "            .orderBy('_1')\n",
    "            .withColumnRenamed('_1', 'id')\n",
    "            .withColumnRenamed('_2', 'type_t')\n",
    "        )\n",
    "\n",
    "df_types_t.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-------+----------------------+-----+-----+\n",
      "|pokemon_id|version_group_id|move_id|pokemon_move_method_id|level|order|\n",
      "+----------+----------------+-------+----------------------+-----+-----+\n",
      "|         1|               1|     14|                     4|    0| null|\n",
      "|         1|               1|     15|                     4|    0| null|\n",
      "|         1|               1|     22|                     1|   13| null|\n",
      "|         1|               1|     33|                     1|    1|    1|\n",
      "|         1|               1|     34|                     4|    0| null|\n",
      "|         1|               1|     36|                     4|    0| null|\n",
      "|         1|               1|     38|                     4|    0| null|\n",
      "|         1|               1|     45|                     1|    1|    2|\n",
      "|         1|               1|     72|                     4|    0| null|\n",
      "|         1|               1|     73|                     1|    7| null|\n",
      "|         1|               1|     74|                     1|   34| null|\n",
      "|         1|               1|     75|                     1|   27| null|\n",
      "|         1|               1|     76|                     1|   48| null|\n",
      "|         1|               1|     76|                     4|    0| null|\n",
      "|         1|               1|     77|                     1|   20| null|\n",
      "|         1|               1|     79|                     1|   41| null|\n",
      "|         1|               1|     92|                     4|    0| null|\n",
      "|         1|               1|     99|                     4|    0| null|\n",
      "|         1|               1|    102|                     4|    0| null|\n",
      "|         1|               1|    104|                     4|    0| null|\n",
      "+----------+----------------+-------+----------------------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- pokemon_id: integer (nullable = true)\n",
      " |-- version_group_id: integer (nullable = true)\n",
      " |-- move_id: integer (nullable = true)\n",
      " |-- pokemon_move_method_id: integer (nullable = true)\n",
      " |-- level: integer (nullable = true)\n",
      " |-- order: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_moves.show(20)\n",
    "df_moves.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|              move_t|\n",
      "+---+--------------------+\n",
      "|  1|(613,[12,13,14,19...|\n",
      "|  2|(613,[13,14,19,21...|\n",
      "|  3|(613,[13,14,19,21...|\n",
      "|  4|(613,[4,6,8,9,13,...|\n",
      "|  5|(613,[4,6,8,9,13,...|\n",
      "|  6|(613,[4,6,8,9,13,...|\n",
      "|  7|(613,[4,7,24,28,3...|\n",
      "|  8|(613,[4,7,24,28,3...|\n",
      "|  9|(613,[4,7,24,28,3...|\n",
      "| 10|(613,[32,80,172,4...|\n",
      "| 11|(613,[80,105,333,...|\n",
      "| 12|(613,[12,15,17,35...|\n",
      "| 13|(613,[39,80,449,5...|\n",
      "| 14|(613,[80,105,333,...|\n",
      "| 15|(613,[13,14,30,35...|\n",
      "| 16|(613,[12,15,16,17...|\n",
      "| 17|(613,[12,15,16,17...|\n",
      "| 18|(613,[12,15,16,17...|\n",
      "| 19|(613,[14,28,32,33...|\n",
      "| 20|(613,[13,14,28,32...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_moves_t = (df_moves\n",
    "              .filter(df_moves.move_id < 10000)   # Remove Shadow Moves exclusive to Coliseum/XD\n",
    "              .select('pokemon_id', (df_moves.move_id-1).alias('move_t'))\n",
    "              )\n",
    "              \n",
    "sparse_size = df_moves_t.selectExpr('count(distinct(move_t))').first()[0]\n",
    "\n",
    "df_moves_t = (df_moves_t\n",
    "            .rdd\n",
    "            .map(lambda row: (row[0], row[1]))\n",
    "            .groupByKey()\n",
    "            .map(lambda row: (row[0], array_to_sparse(sparse_size, row[1])))\n",
    "            .toDF()\n",
    "            .orderBy('_1')\n",
    "            .withColumnRenamed('_1', 'id')\n",
    "            .withColumnRenamed('_2', 'move_t')\n",
    "        )\n",
    "\n",
    "df_moves_t.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------+----+\n",
      "|pokemon_id|ability_id|is_hidden|slot|\n",
      "+----------+----------+---------+----+\n",
      "|         1|        65|        0|   1|\n",
      "|         1|        34|        1|   3|\n",
      "|         2|        65|        0|   1|\n",
      "|         2|        34|        1|   3|\n",
      "|         3|        65|        0|   1|\n",
      "|         3|        34|        1|   3|\n",
      "|         4|        66|        0|   1|\n",
      "|         4|        94|        1|   3|\n",
      "|         5|        66|        0|   1|\n",
      "|         5|        94|        1|   3|\n",
      "|         6|        66|        0|   1|\n",
      "|         6|        94|        1|   3|\n",
      "|         7|        67|        0|   1|\n",
      "|         7|        44|        1|   3|\n",
      "|         8|        67|        0|   1|\n",
      "|         8|        44|        1|   3|\n",
      "|         9|        67|        0|   1|\n",
      "|         9|        44|        1|   3|\n",
      "|        10|        19|        0|   1|\n",
      "|        10|        50|        1|   3|\n",
      "+----------+----------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- pokemon_id: integer (nullable = true)\n",
      " |-- ability_id: integer (nullable = true)\n",
      " |-- is_hidden: integer (nullable = true)\n",
      " |-- slot: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_abilities.show(20)\n",
    "df_abilities.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|           ability_t|\n",
      "+---+--------------------+\n",
      "|  1|(191,[33,64],[1.0...|\n",
      "|  2|(191,[33,64],[1.0...|\n",
      "|  3|(191,[33,64],[1.0...|\n",
      "|  4|(191,[65,93],[1.0...|\n",
      "|  5|(191,[65,93],[1.0...|\n",
      "|  6|(191,[65,93],[1.0...|\n",
      "|  7|(191,[43,66],[1.0...|\n",
      "|  8|(191,[43,66],[1.0...|\n",
      "|  9|(191,[43,66],[1.0...|\n",
      "| 10|(191,[18,49],[1.0...|\n",
      "| 11|    (191,[60],[1.0])|\n",
      "| 12|(191,[13,109],[1....|\n",
      "| 13|(191,[18,49],[1.0...|\n",
      "| 14|    (191,[60],[1.0])|\n",
      "| 15|(191,[67,96],[1.0...|\n",
      "| 16|(191,[50,76,144],...|\n",
      "| 17|(191,[50,76,144],...|\n",
      "| 18|(191,[50,76,144],...|\n",
      "| 19|(191,[49,54,61],[...|\n",
      "| 20|(191,[49,54,61],[...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_abilities_t = (df_abilities\n",
    "              .filter(df_abilities.ability_id < 10000)   # Invalid\n",
    "              .select('pokemon_id', (df_abilities.ability_id-1).alias('ability_t'))\n",
    "              )\n",
    "              \n",
    "sparse_size = df_abilities_t.selectExpr('count(distinct(ability_t))').first()[0]\n",
    "\n",
    "df_abilities_t = (df_abilities_t\n",
    "            .rdd\n",
    "            .map(lambda row: (row[0], row[1]))\n",
    "            .groupByKey()\n",
    "            .map(lambda row: (row[0], array_to_sparse(sparse_size, row[1])))\n",
    "            .toDF()\n",
    "            .orderBy('_1')\n",
    "            .withColumnRenamed('_1', 'id')\n",
    "            .withColumnRenamed('_2', 'ability_t')\n",
    "        )\n",
    "\n",
    "df_abilities_t.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Species\n",
    "\n",
    "We can extract several `color`, `shape`, and `habitat` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------+-----------------------+------------------+--------+--------+----------+-----------+------------+--------------+-------+-------------+----------------------+--------------+----------------+-----+--------------+\n",
      "| id|identifier|generation_id|evolves_from_species_id|evolution_chain_id|color_id|shape_id|habitat_id|gender_rate|capture_rate|base_happiness|is_baby|hatch_counter|has_gender_differences|growth_rate_id|forms_switchable|order|conquest_order|\n",
      "+---+----------+-------------+-----------------------+------------------+--------+--------+----------+-----------+------------+--------------+-------+-------------+----------------------+--------------+----------------+-----+--------------+\n",
      "|  1| bulbasaur|            1|                   null|                 1|       5|       8|         3|          1|          45|            70|      0|           20|                     0|             4|               0|    1|          null|\n",
      "|  2|   ivysaur|            1|                      1|                 1|       5|       8|         3|          1|          45|            70|      0|           20|                     0|             4|               0|    2|          null|\n",
      "|  3|  venusaur|            1|                      2|                 1|       5|       8|         3|          1|          45|            70|      0|           20|                     1|             4|               1|    3|          null|\n",
      "|  4|charmander|            1|                   null|                 2|       8|       6|         4|          1|          45|            70|      0|           20|                     0|             4|               0|    4|           109|\n",
      "|  5|charmeleon|            1|                      4|                 2|       8|       6|         4|          1|          45|            70|      0|           20|                     0|             4|               0|    5|           110|\n",
      "|  6| charizard|            1|                      5|                 2|       8|       6|         4|          1|          45|            70|      0|           20|                     0|             4|               1|    6|           111|\n",
      "+---+----------+-------------+-----------------------+------------------+--------+--------+----------+-----------+------------+--------------+-------+-------------+----------------------+--------------+----------------+-----+--------------+\n",
      "only showing top 6 rows\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- identifier: string (nullable = true)\n",
      " |-- generation_id: integer (nullable = true)\n",
      " |-- evolves_from_species_id: integer (nullable = true)\n",
      " |-- evolution_chain_id: integer (nullable = true)\n",
      " |-- color_id: integer (nullable = true)\n",
      " |-- shape_id: integer (nullable = true)\n",
      " |-- habitat_id: integer (nullable = true)\n",
      " |-- gender_rate: integer (nullable = true)\n",
      " |-- capture_rate: integer (nullable = true)\n",
      " |-- base_happiness: integer (nullable = true)\n",
      " |-- is_baby: integer (nullable = true)\n",
      " |-- hatch_counter: integer (nullable = true)\n",
      " |-- has_gender_differences: integer (nullable = true)\n",
      " |-- growth_rate_id: integer (nullable = true)\n",
      " |-- forms_switchable: integer (nullable = true)\n",
      " |-- order: integer (nullable = true)\n",
      " |-- conquest_order: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_species.show(6)\n",
    "df_species.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------+--------------+\n",
      "| id|       color_t|        shape_t|     habitat_t|\n",
      "+---+--------------+---------------+--------------+\n",
      "|  1|(10,[4],[1.0])| (14,[7],[1.0])|(10,[2],[1.0])|\n",
      "|  2|(10,[4],[1.0])| (14,[7],[1.0])|(10,[2],[1.0])|\n",
      "|  3|(10,[4],[1.0])| (14,[7],[1.0])|(10,[2],[1.0])|\n",
      "|  4|(10,[7],[1.0])| (14,[5],[1.0])|(10,[3],[1.0])|\n",
      "|  5|(10,[7],[1.0])| (14,[5],[1.0])|(10,[3],[1.0])|\n",
      "|  6|(10,[7],[1.0])| (14,[5],[1.0])|(10,[3],[1.0])|\n",
      "|  7|(10,[1],[1.0])| (14,[5],[1.0])|(10,[8],[1.0])|\n",
      "|  8|(10,[1],[1.0])| (14,[5],[1.0])|(10,[8],[1.0])|\n",
      "|  9|(10,[1],[1.0])| (14,[5],[1.0])|(10,[8],[1.0])|\n",
      "| 10|(10,[4],[1.0])| (14,[1],[1.0])|(10,[1],[1.0])|\n",
      "| 11|(10,[4],[1.0])| (14,[1],[1.0])|(10,[1],[1.0])|\n",
      "| 12|(10,[8],[1.0])|(14,[12],[1.0])|(10,[1],[1.0])|\n",
      "| 13|(10,[2],[1.0])| (14,[1],[1.0])|(10,[1],[1.0])|\n",
      "| 14|(10,[9],[1.0])| (14,[1],[1.0])|(10,[1],[1.0])|\n",
      "| 15|(10,[9],[1.0])|(14,[12],[1.0])|(10,[1],[1.0])|\n",
      "| 16|(10,[2],[1.0])| (14,[8],[1.0])|(10,[1],[1.0])|\n",
      "| 17|(10,[2],[1.0])| (14,[8],[1.0])|(10,[1],[1.0])|\n",
      "| 18|(10,[2],[1.0])| (14,[8],[1.0])|(10,[1],[1.0])|\n",
      "| 19|(10,[6],[1.0])| (14,[7],[1.0])|(10,[2],[1.0])|\n",
      "| 20|(10,[2],[1.0])| (14,[7],[1.0])|(10,[2],[1.0])|\n",
      "+---+--------------+---------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_species_t = (df_species \n",
    "              .select('id',\n",
    "                      (df_species.color_id-1).alias('color_t'),\n",
    "                      (df_species.shape_id-1).alias('shape_t'),\n",
    "                      (df_species.habitat_id-1).alias('habitat_t'))\n",
    "                .fillna({'habitat_t': 9}) # Empty for some pokemon\n",
    "              )\n",
    "              \n",
    "sparse_size_color = df_species_t.selectExpr('count(distinct(color_t))').first()[0]\n",
    "sparse_size_shape = df_species_t.selectExpr('count(distinct(shape_t))').first()[0]\n",
    "sparse_size_habitat = df_species_t.selectExpr('count(distinct(habitat_t))').first()[0]\n",
    "\n",
    "df_species_t = (df_species_t\n",
    "            .rdd\n",
    "            .map(lambda row: (row[0],\n",
    "                              #row[1],\n",
    "                              Vectors.sparse(sparse_size_color, {row[1]: 1.0}),\n",
    "                              Vectors.sparse(sparse_size_shape, {row[2]: 1.0}),\n",
    "                              Vectors.sparse(sparse_size_habitat, {row[3]: 1.0}),\n",
    "                             ))\n",
    "            .toDF()\n",
    "            .orderBy('_1')\n",
    "            .withColumnRenamed('_1', 'id')\n",
    "            .withColumnRenamed('_2', 'color_t')\n",
    "            .withColumnRenamed('_3', 'shape_t')\n",
    "            .withColumnRenamed('_4', 'habitat_t')\n",
    "        )\n",
    "\n",
    "df_species_t.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finale\n",
    "\n",
    "Join and combine EVERYTHING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- height_t: double (nullable = true)\n",
      " |-- weight_t: double (nullable = true)\n",
      " |-- attack_t: double (nullable = true)\n",
      " |-- defense_t: double (nullable = true)\n",
      " |-- special_attack_t: double (nullable = true)\n",
      " |-- special_defense_t: double (nullable = true)\n",
      " |-- speed_t: double (nullable = true)\n",
      " |-- type_t: vector (nullable = true)\n",
      " |-- move_t: vector (nullable = true)\n",
      " |-- ability_t: vector (nullable = true)\n",
      " |-- color_t: vector (nullable = true)\n",
      " |-- shape_t: vector (nullable = true)\n",
      " |-- habitat_t: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_combined = (df_pokemon_t\n",
    "               .join(df_stats_t, 'id', 'left')\n",
    "               .join(df_types_t, 'id', 'left')\n",
    "               .join(df_moves_t, 'id', 'left')\n",
    "               .join(df_abilities_t, 'id', 'left')\n",
    "               .join(df_species_t, 'id', 'left')\n",
    "               .cache()\n",
    "               )\n",
    "\n",
    "df_combined.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before combining vectors, all vectors must be indexed with `VectorIndexer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- height_t: double (nullable = true)\n",
      " |-- weight_t: double (nullable = true)\n",
      " |-- attack_t: double (nullable = true)\n",
      " |-- defense_t: double (nullable = true)\n",
      " |-- special_attack_t: double (nullable = true)\n",
      " |-- special_defense_t: double (nullable = true)\n",
      " |-- speed_t: double (nullable = true)\n",
      " |-- type_tv: vector (nullable = true)\n",
      " |-- move_tv: vector (nullable = true)\n",
      " |-- ability_tv: vector (nullable = true)\n",
      " |-- color_tv: vector (nullable = true)\n",
      " |-- shape_tv: vector (nullable = true)\n",
      " |-- habitat_tv: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# http://stackoverflow.com/a/32984795\n",
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler\n",
    "\n",
    "#col_names = df_combined.schema.names[1:]\n",
    "\n",
    "for col_name in df_combined.schema.names[8:]:\n",
    "    vector_indexer = VectorIndexer(inputCol=col_name, outputCol=col_name+'v')\n",
    "\n",
    "    df_combined = (vector_indexer\n",
    "                    .fit(df_combined)\n",
    "                    .transform(df_combined)\n",
    "                       .drop(col_name))\n",
    "\n",
    "df_combined.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all the values into a single `features` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "|  1|(863,[0,1,2,3,4,5...|\n",
      "|  2|(863,[0,1,2,3,4,5...|\n",
      "|  3|(863,[0,1,2,3,4,5...|\n",
      "|  4|(863,[0,1,2,3,4,5...|\n",
      "|  5|(863,[0,1,2,3,4,5...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=df_combined.schema.names[1:], outputCol=\"features\")\n",
    "df_features = (assembler\n",
    "               .transform(df_combined)\n",
    "               .select(['id', 'features'])\n",
    "               .orderBy('id')\n",
    "               .cache()\n",
    "               )\n",
    "\n",
    "df_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            features|        pca_features|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|(863,[0,1,2,3,4,5...|[-1.2328845199078...|\n",
      "|  2|(863,[0,1,2,3,4,5...|[-1.2587694818593...|\n",
      "|  3|(863,[0,1,2,3,4,5...|[-1.8688853599180...|\n",
      "|  4|(863,[0,1,2,3,4,5...|[-5.5015619452006...|\n",
      "|  5|(863,[0,1,2,3,4,5...|[-5.3619352981827...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "0.127711510102\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "pca = PCA(k=50, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "model = pca.fit(df_features)\n",
    "df_features = model.transform(df_features)\n",
    "\n",
    "df_features.show(5)\n",
    "print sum(model.explainedVariance[0:2])   # % of model explained by first 3 components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempted to expand to polynomial space to see if we get better model. However, `863^2/2 ~ 372384 ` and PCA feature limit is apparently `65535`. Code left for posterity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from pyspark.ml.feature import PolynomialExpansion\n",
    "\n",
    "#poly = PolynomialExpansion(degree=2, inputCol=\"features\", outputCol=\"polyfeatures\")\n",
    "#df_features = poly.transform(df_features)\n",
    "#\n",
    "#pca = PCA(k=50, inputCol=\"polyfeatures\", outputCol=\"pca_polyfeatures\")\n",
    "#model = pca.fit(df_features)\n",
    "#df_features = model.transform(df_features)\n",
    "#\n",
    "#df_features.show(5)\n",
    "#print sum(model.explainedVariance[0:2])   # % of model explained by first 3 components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export as CSV. Spark csv export is apparently ugly due to the `DenseVectors`, so use `pandas` CSV export instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_features.select('id',df_features.pca_features).repartition(1).write.format('csv').save(\"pokemon_pca\", header=True, mode=\"overwrite\")\n",
    "#df_features.select('id','pca_features').coalesce(1).write.format('json').save(\"pokemon_pca\", header=True, mode=\"overwrite\")\n",
    "\n",
    "import pandas as pd\n",
    "df_features.select('id','pca_features').toPandas().to_csv('pokemon_pca.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MIT License (MIT)\n",
    "\n",
    "Copyright (c) 2016 Max Woolf\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
